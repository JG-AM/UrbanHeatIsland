{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532ea17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Useful for running command line within python\n",
    "import pandas as pd ## Useful for data manipulation\n",
    "\n",
    "import torch ## Pytorch is the deep learning library that we will be using\n",
    "import torch.nn as nn # Neural network module\n",
    "\n",
    "import torchmetrics ## Torchmetrics is a library that contains metrics for evaluating models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from torchmetrics.regression import MeanSquaredError\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.nn import ReLU, Sequential\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9f9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_range(inputs,historic, y1=1,y0=-1):\n",
    "    n_inputs=inputs.shape[1]\n",
    "    names_inputs=historic.columns\n",
    "    slope=torch.zeros(n_inputs)\n",
    "    b_array=torch.zeros(n_inputs)\n",
    "    for i in range(n_inputs):\n",
    "        x1=historic[names_inputs[i+1]][1]\n",
    "        x0=historic[names_inputs[i+1]][0]\n",
    "        slope[i]=(y1-y0)/(x1-x0)\n",
    "        b_array[i]=-x0*slope[i]+y0\n",
    "                    \n",
    "    return inputs*slope+b_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e77da0",
   "metadata": {},
   "source": [
    "Construct the training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9766c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are creating a custom dataset class\n",
    "class WeatherData(Dataset):\n",
    "\n",
    "    def __init__(self, csv_inputs, csv_outputs,csv_historic,y1=1,y0=-1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_inputs (string): Path to the csv file of the inputs of the NN.\n",
    "            csv_outputs (string): Path to the csv file of temperatures 24 and 48 hours after measurement .\n",
    "            csv_historic (string): Path to the csv file of historic weather data.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_data = pd.read_csv(csv_inputs)\n",
    "        output_data= pd.read_csv(csv_outputs)\n",
    "        output_data=output_data[\"temperature_24\"]\n",
    "        historic= pd.read_csv(csv_historic)\n",
    "        \n",
    "        \n",
    "        ## Transform input data\n",
    "        n=len(input_data)\n",
    "        minutes=[datetime.datetime.strptime(input_data[\"local_time\"][i], '%Y-%m-%d %H:%M:%S').minute for i in range(n)]\n",
    "        input_data[\"local_time\"]=minutes\n",
    "        self.inputs=transform_to_range(torch.tensor(input_data.values.astype(np.float32)),historic,y1=y1,y0=y0)\n",
    "        \n",
    "        #Transform output data\n",
    "        x1_temperature=historic[\"Air_Temperature_in_degrees_C\"][1]\n",
    "        x0_temperature=historic[\"Air_Temperature_in_degrees_C\"][0]\n",
    "        slope_temperature=(y1-y0)/(x1_temperature-x0_temperature)\n",
    "        b_temperature=-x0_temperature*slope_temperature+y0\n",
    "        self.outputs=torch.tensor(output_data.values.astype(np.float32))*slope_temperature+b_temperature\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        return self.inputs[idx],self.outputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ca9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_86077=\"./data/cleaned_data/86077_aws_plumber_data_inputs.csv\"\n",
    "output_86077=\"./data/cleaned_data/86077_aws_plumber_data_outputs.csv\"\n",
    "historic_path=\"./data/cleaned_data/Historical_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73f0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=WeatherData(input_86077,output_86077,historic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd0a44",
   "metadata": {},
   "source": [
    "The data is order chronologically, so the division in terms of training, validation, and test should not be random. Instead, the data will be partition in 3 blocks. The blocks are ordered in chronological order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde6ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data=len(dataset)\n",
    "split=[0.8,0.1,0.1]\n",
    "n_train=int(np.floor(split[0]*n_data))\n",
    "n_val=int(np.floor(split[1]*n_data))\n",
    "\n",
    "trainset=torch.utils.data.Subset(dataset, range(n_train))\n",
    "valset=torch.utils.data.Subset(dataset, range(n_train,n_train+n_val))\n",
    "testset =torch.utils.data.Subset(dataset, range(n_train,n_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2665cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch size of 16.\n",
    "BATCH_SIZE = 64 #Just indicate the batch size\n",
    "\n",
    "# Create Dataloaders \n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7ea7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are going to use the PyTorch's nn.LSTM() method\n",
    "class WeatherLSTM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n, learning_rate=1e-2, traindataloader=None, valdataloader=None, testdataloader=None): \n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        self.n=n\n",
    "        self.lstm = nn.LSTM(input_size=11, hidden_size=1) \n",
    "        ## The input size corresponds to the number of features of each measurement\n",
    "        ## The hidden size gives the size of the output. In our case 2. The 24 and 48 temperature prediction\n",
    "        \n",
    "        # define learning rate\n",
    "        self.learning_rate = learning_rate ## Just the numerical value of the learning rate\n",
    "\n",
    "        # define loss function (Huber loss). This function is robust against outliers\n",
    "        self.loss_fun = nn.HuberLoss() ## The function to calculate the loss\n",
    "        \n",
    "        # We calculate the mean square error as a perfomance metric\n",
    "        \n",
    "        self.train_mse = MeanSquaredError()\n",
    "        self.val_mse = MeanSquaredError()\n",
    "        self.test_mse = MeanSquaredError()\n",
    "        \n",
    "         # Define dataloaders\n",
    "        self.traindataloader = traindataloader ### I am just passing the corresponding dataloader\n",
    "        self.valdataloader = valdataloader   ### I am just passing the corresponding dataloader\n",
    "        self.testdataloader = testdataloader ### I am just passing the corresponding dataloader\n",
    "         \n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(len(x), self.n)\n",
    "        lstm_out, temp = self.lstm(x)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Make predictions\n",
    "        x,y=batch ## The batch should come from the CovidDataset class\n",
    "        # Do the prediction\n",
    "        \n",
    "        y_hat=self.forward(x) ## This predicts the results from x\n",
    "\n",
    "        # Calculate the loss\n",
    "        # Apply the loss function here\n",
    "        loss=self.loss_fun(y_hat,y) ## Calculate the loss with the function declared in this class\n",
    "        ## Update the MSE\n",
    "        self.train_mse.update(y_hat, y)\n",
    "        \n",
    "        \n",
    "        # Record accuracy and loss\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        # You may add more logs as you think necessary\n",
    "         \n",
    "        ### In the next two lines the history of the loss and accuracy is logged\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_mse\", self.train_mse, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        # Return the loss\n",
    "        return loss ## \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        ### I am going to reuse the code for the training\n",
    "        x,y=batch \n",
    "        y_hat=self.forward(x) ## This predicts the results from x\n",
    "\n",
    "        # Compute loss for each batch\n",
    "        loss=self.loss_fun(y_hat,y) ## Calculate the loss \n",
    "        ## Update the MSE\n",
    "        self.val_mse.update(y_hat, y)\n",
    "\n",
    "\n",
    "        # Record accuracy and loss\n",
    "        # Log anything you think necessary\n",
    "        ### Save the logs for the loss and for the accuracy\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_mse\", self.val_mse, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        x,y=batch \n",
    "        y_hat=self.forward(x) ## This predicts the results from x\n",
    "\n",
    "        # Compute loss for each batch\n",
    "        loss=self.loss_fun(y_hat,y) ## Calculate the loss \n",
    "\n",
    "        self.test_mse.update(y_hat, y) \n",
    "\n",
    "        # Record accuracy and loss\n",
    "        # Log anything you think necessary\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_mse\", self.test_mse, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \n",
    "        x,y=batch\n",
    "        # Do prediction\n",
    "        y_hat=self.forward(x)\n",
    "\n",
    "        return y_hat,y,x # Return prediction, actual value and inputs  \n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): \n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate) \n",
    "\n",
    "    \n",
    "     ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # return the train dataloader\n",
    "        return  self.traindataloader ## Just return the traindataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # return the validation dataloader\n",
    "        return self.valdataloader  ### Just return the valdataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # return the test dataloader\n",
    "        return self.testdataloader  ## Just return the testdataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50fad59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model by creating instance from Model class\n",
    "lstm_model = WeatherLSTM(11, traindataloader=trainloader, valdataloader=valloader, testdataloader=testloader)\n",
    "\n",
    "# Display a summary of the model's architecture\n",
    "#summary(lstm_model, (1,11)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e03906fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=torch.tensor([[1,2,3,4,5,6,7,8,9,10,11.0],[2.0,2.0,3,4,5,6,7,8,9,10,11.0],[2.0,2.0,3,4,5,8.0,7,8,9,10,11.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56bdcd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm=nn.LSTM(input_size=11, hidden_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2a48199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa=torch.tensor([[0.,1], [0.5,2]])\n",
    "outputs,temp=test_lstm(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3501844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0470e-10], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7df30e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0026])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model(aa).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "880f659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "lstm_callback = ModelCheckpoint(\n",
    "        monitor='val_mse',\n",
    "        dirpath='.\\\\logs_lstm',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        every_n_epochs=1)\n",
    "# Monitor the validation mean square eror\n",
    "# Specify a directory for checkpoints\n",
    "# save the one best model\n",
    "# Save the min. monitored quantity\n",
    "# Monitor every 1 epoch\n",
    "\n",
    "\n",
    "# Create a Trainer\n",
    "n_epochs=100  ## I am defining the maximum number of epochs\n",
    "lstm_trainer = pl.Trainer(accelerator = \"auto\",\n",
    "                  max_epochs = n_epochs,\n",
    "                  callbacks = [TQDMProgressBar(refresh_rate=20),lstm_callback],\n",
    "                  logger = CSVLogger(save_dir=\".\\\\logs_lstm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0034d7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | lstm      | LSTM             | 56    \n",
      "1 | loss_fun  | HuberLoss        | 0     \n",
      "2 | train_mse | MeanSquaredError | 0     \n",
      "3 | val_mse   | MeanSquaredError | 0     \n",
      "4 | test_mse  | MeanSquaredError | 0     \n",
      "-----------------------------------------------\n",
      "56        Trainable params\n",
      "0         Non-trainable params\n",
      "56        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c37e7c21014c11979b1c7798264c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Predictions and targets are expected to have the same shape, but got torch.Size([1]) and torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlstm_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[1;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[0;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[0;32m    563\u001b[0m )\n\u001b[0;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    567\u001b[0m     ckpt_path,\n\u001b[0;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    570\u001b[0m )\n\u001b[1;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1021\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1021\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1050\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1047\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1050\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m     previous_dataloader_idx \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:376\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[0;32m    375\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 376\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    380\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:293\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    296\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:393\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[18], line 74\u001b[0m, in \u001b[0;36mWeatherLSTM.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     72\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fun(y_hat,y) \u001b[38;5;66;03m## Calculate the loss \u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m## Update the MSE\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Record accuracy and loss\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Log anything you think necessary\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m### Save the logs for the loss and for the accuracy\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\torchmetrics\\metric.py:467\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[0;32m    460\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device corresponds to the device of the input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_cpu:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\torchmetrics\\metric.py:457\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 457\u001b[0m         update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\torchmetrics\\regression\\mse.py:101\u001b[0m, in \u001b[0;36mMeanSquaredError.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     sum_squared_error, n_obs \u001b[38;5;241m=\u001b[39m \u001b[43m_mean_squared_error_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_squared_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sum_squared_error\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_obs\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\torchmetrics\\functional\\regression\\mse.py:34\u001b[0m, in \u001b[0;36m_mean_squared_error_update\u001b[1;34m(preds, target, num_outputs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean_squared_error_update\u001b[39m(preds: Tensor, target: Tensor, num_outputs: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update and returns variables required to compute Mean Squared Error.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    Check for same shape of input tensors.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     \u001b[43m_check_same_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     _check_data_shape_to_num_outputs(preds, target, num_outputs, allow_1d_reshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_outputs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ws1\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:42\u001b[0m, in \u001b[0;36m_check_same_shape\u001b[1;34m(preds, target)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that predictions and target have the same shape, else raise error.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and targets are expected to have the same shape, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Predictions and targets are expected to have the same shape, but got torch.Size([1]) and torch.Size([64])."
     ]
    }
   ],
   "source": [
    "lstm_trainer.fit(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bac852d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000, -0.3252,  0.8400, -1.0000, -0.0905, -1.0000, -0.2419,  0.2027,\n",
       "          0.0742, -1.0000, -1.0000]),\n",
       " tensor(-0.1626))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5378ead1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000, -0.3252,  0.8400, -1.0000, -0.0905, -1.0000, -0.2419,  0.2027,\n",
       "          0.0742, -1.0000, -1.0000]),\n",
       " tensor(-0.1626))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec4098fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000, -0.3252,  0.8400, -1.0000, -0.0905, -1.0000, -0.2419,  0.2027,\n",
       "           0.0742, -1.0000, -1.0000],\n",
       "         [-1.0000, -0.3252,  0.8600,  0.0000, -0.0952, -1.0000, -0.2543,  0.2017,\n",
       "           0.0751, -1.0000, -1.0000]]),\n",
       " tensor([-0.1626, -0.1789]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0bdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
